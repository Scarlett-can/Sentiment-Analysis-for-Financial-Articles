{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numba.decorators._jit.<locals>.wrapper(func)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function numba.decorators._jit.<locals>.wrapper(func)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function numba.decorators._jit.<locals>.wrapper(func)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function numba.decorators._jit.<locals>.wrapper(func)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function numba.decorators._jit.<locals>.wrapper(func)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function numba.decorators._jit.<locals>.wrapper(func)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10856436a3242af97aea8b58a1fc695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.8030929565429688\n"
     ]
    }
   ],
   "source": [
    "# import packages #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from numba import jit\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for removing accented characters\n",
    "import unicodedata\n",
    "\n",
    "# for expanding contractions\n",
    "from contractions import CONTRACTION_MAP\n",
    "from pycontractions import Contractions\n",
    "\n",
    "# for lemmatization\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# multiple outputs in one chunk (jupyter notebook)\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "\n",
    "# load corpus #\n",
    "def load_corpus(articles):\n",
    "    '''\n",
    "    Load corpus, 100 articles.\n",
    "    '''\n",
    "    df = pd.read_csv(articles)\n",
    "    df = df.to_dict()\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# load Loughran & McDonald finance dictionary #\n",
    "jit(nopython = True, parallel = True)\n",
    "def load_LM_dict(dictionary, pos = True, neg = True):\n",
    "    '''\n",
    "    Load positive/negative words list. Adopt dictionary to improve efficiency.\n",
    "    '''\n",
    "    if pos: \n",
    "        poslst = pd.read_excel(dictionary, sheet_name = \"Positive\", header = None)[0].tolist()\n",
    "        posdict = {pos:1 for pos in poslst}\n",
    "    if neg:\n",
    "        neglst = pd.read_excel(dictionary, sheet_name = \"Negative\", header = None)[0].tolist()\n",
    "        negdict = {neg:1 for neg in neglst}\n",
    "    return posdict, negdict\n",
    "\n",
    "\n",
    "\n",
    "# text pre-processing #\n",
    "\n",
    "## remove HTML tags ##\n",
    "def remove_html_tags(text):\n",
    "    '''\n",
    "    Remove the noise, html tags.\n",
    "    '''\n",
    "    bs = BeautifulSoup(text, 'html.parser')\n",
    "    text = bs.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "## remove accented characters ##\n",
    "def remove_accented_characters(text):\n",
    "    '''\n",
    "    Convert the corpus into English pharsings.\n",
    "    '''\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "## expand contractions ##\n",
    "def expand_contractions(text, contraction_mapping = CONTRACTION_MAP):\n",
    "    '''\n",
    "    Expand contractions. If necessary, add additional contraction-expansion pairs into contraction.py.\n",
    "    '''\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags = re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "            if contraction_mapping.get(match)\\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\",\"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "## remove special characters ##\n",
    "def remove_special_characters(text):\n",
    "    '''\n",
    "    Remove puntuations, numbers, redundant spaces.\n",
    "    '''\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(' +' ,' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "## tokenize words ##\n",
    "def tokenize_words(text, wt = False, wpt = False, wst = False, tbwt = False, tkt = False):\n",
    "    '''\n",
    "    Multiple tokenizers are available in NLTK library. Create a tokenizer container.\n",
    "    '''\n",
    "    if wt:\n",
    "        return word_tokenize(text)\n",
    "    if wpt:\n",
    "        return WordPunctTokenizer().tokenize(text)\n",
    "    if wst:\n",
    "        return WhitespaceTokenizer().tokenize(text)\n",
    "    if tbwt:\n",
    "        return TreebankWordTokenizer().tokenize(text)\n",
    "    if tkt:\n",
    "        return ToktokTokenizer().tokenize(text)\n",
    "    \n",
    "    \n",
    "## lemmatization ##\n",
    "def wordnet_pos(tag):\n",
    "    '''\n",
    "    Obtain Part-Of-Speech tags (in lowercase).\n",
    "    '''\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "jit(nopython = True, parallel = True)\n",
    "def lemmatization(tokens):\n",
    "    '''\n",
    "    Convert multiple word forms backs into lemmas.\n",
    "    '''\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas = [] \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        pos = wordnet_pos(tag) or wordnet.NOUN\n",
    "        lemmas.append(wnl.lemmatize(token, pos)) \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "## remove stopwords ##\n",
    "jit(nopython = True, parallel = True)\n",
    "def remove_stopwords(stopwords_list, tokens):\n",
    "    '''\n",
    "    Remove stopwords in the corpus.\n",
    "    '''\n",
    "    cleaned_tokens = list(filter(lambda x: x not in stopwords_list, tokens))\n",
    "#     cleaned_tokens = [token for token in tokens if not token in stopwords_list]\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n",
    "\n",
    "# build a text normalizer #\n",
    "jit(nopython = True, parallel = True)\n",
    "def normalizer(text, stopwords_list, wtk = False, wptk = False, wstk = False, tbwtk = False, tktk = False,\n",
    "               no_accented_chars = True, no_contracted_chars = True, no_lammas = True, no_stopwords = False):\n",
    "    '''\n",
    "    Build a text normalizer. Assign the Boolean value True to the applied tokenizer. \n",
    "    By defualt, do not remove accented characters, do not expand contractions, \n",
    "    do not do words lemmatization and remove stopwords.\n",
    "    '''\n",
    "    text = remove_html_tags(text)\n",
    "    if not no_accented_chars:\n",
    "        text = remove_accented_characters(text)   \n",
    "    if not no_contracted_chars:\n",
    "        text = expand_contractions(text, contraction_mapping = CONTRACTION_MAP)\n",
    "    text = remove_special_characters(text).lower()\n",
    "    tokens = tokenize_words(text, wt = wtk, wpt = wptk, wst = wstk, tbwt = tbwtk, tkt = tktk) \n",
    "    if not no_lammas:\n",
    "        tokens = lemmatization(tokens)    \n",
    "    if not no_stopwords:\n",
    "        tokens = remove_stopwords(stopwords_list, tokens)   \n",
    "    normalized_tokens = [token.upper() for token in tokens if len(token) > 1]\n",
    "    return normalized_tokens\n",
    "\n",
    "\n",
    "\n",
    "# sentiment analysis #\n",
    "jit(nopython = True, parallel = True)\n",
    "def sentiment(words, posdict, negdict):\n",
    "    '''\n",
    "    Count positive/negative words. \n",
    "    Calculate sentiment score.\n",
    "    Sentiment score is positive words fraction minus negative words fraction in the article content\n",
    "    '''\n",
    "    poswds, negwds = 0, 0\n",
    "    poslst, neglst = [], []\n",
    "    totwds = len(words)\n",
    "    for word in words:\n",
    "        try:\n",
    "            posdict[word] == 1\n",
    "            poswds += 1\n",
    "            poslst.append(word)\n",
    "        except:\n",
    "            try:\n",
    "                negdict[word] == 1\n",
    "                negwds += 1\n",
    "                neglst.append(word)\n",
    "            except:\n",
    "                continue        \n",
    "    sentisc = (poswds - negwds)/totwds \n",
    "    return poswds, negwds, totwds, sentisc, poslst, neglst\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # record initial time\n",
    "    time_start = time.time()\n",
    "    \n",
    "    # load the corpus, dictionary\n",
    "    corpus = load_corpus(\"articles.csv\")\n",
    "    posdict, negdict = load_LM_dict(\"LoughranMcDonald_SentimentWordLists_2018.xlsx\")\n",
    "    \n",
    "    # update stopwords list, remove overlap: stopwords corpus, LM dictionary\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stopwords_list = [word for word in stop_words if not word.upper() in posdict and not word.upper() in negdict]\n",
    "    \n",
    "    # prepare a dictionary for the output\n",
    "    opdict = {'article_id': list(corpus['aid'].values()), 'pos_word': [], 'neg_word': [], \n",
    "              'total_word': [], 'sentiment': [], 'pos_lst': [], 'neg_lst': []}\n",
    "    \n",
    "    jit(nopython = True, parallel = True)\n",
    "    def output():\n",
    "        for i in tqdm_notebook(range(len(corpus['aid']))):\n",
    "\n",
    "            # text pre-processing\n",
    "            text = corpus['content'][i]\n",
    "            normalized_tokens = normalizer(text, stopwords_list, wtk = True)\n",
    "\n",
    "            # sentiment analysis\n",
    "            poswds, negwds, totwds, sentisc, poslst, neglst = sentiment(normalized_tokens, posdict, negdict)\n",
    "\n",
    "            # update output\n",
    "            opdict['pos_word'].append(poswds)\n",
    "            opdict['neg_word'].append(negwds)\n",
    "            opdict['total_word'].append(totwds)\n",
    "            opdict['sentiment'].append(sentisc)\n",
    "            opdict['pos_lst'].append(poslst)\n",
    "            opdict['neg_lst'].append(neglst)\n",
    "        \n",
    "        return pd.DataFrame(opdict)\n",
    "    \n",
    "    opdf = output()\n",
    "    opdf[['article_id', 'pos_word', 'neg_word', 'total_word', 'sentiment']].to_csv(\"results_nltk.csv\", index = False)\n",
    "    \n",
    "    \n",
    "    # record finish time\n",
    "    time_end = time.time()\n",
    "    print(time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
